{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2a7ed7-005d-4ba4-970d-beb672f3ba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size=8\n",
    "batch_size=8\n",
    "learning_rate= 3e-4\n",
    "max_iters = 100000\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d73c44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open('wizard_of_oz.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars=sorted(set(text))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a3b81d-ac17-4151-9eb5-4f318a1efc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int={ch:i for i,ch in enumerate(chars)}#here we did mapping \n",
    "int_to_string={i:ch for i,ch in enumerate(chars)}\n",
    "encode=lambda s:[string_to_int[c] for c in s] # lambda arguments: expression\n",
    "decode=lambda l:''.join([int_to_string[i] for i in l ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e45387-d351-40cb-a495-1ad8fdcce175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91, 48, 65, 62, 1, 44, 75, 72, 67, 62, 60, 77, 1, 35, 78, 77, 62, 71, 59, 62, 75, 64, 1, 62, 30, 72, 72, 68, 1, 72, 63, 1, 32, 72, 75, 72, 77, 65, 82, 1, 58, 71, 61, 1, 77, 65, 62, 1, 51, 66, 83, 58, 75, 61, 1, 66, 71, 1, 43, 83, 0, 1, 1, 1, 1, 0, 48, 65, 66, 76, 1, 62, 59, 72, 72, 68, 1, 66, 76, 1, 63, 72, 75, 1, 77, 65, 62, 1, 78, 76, 62, 1, 72, 63, 1, 58, 71, 82, 72, 71]\n"
     ]
    }
   ],
   "source": [
    "print(encode(text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a456d62a-c3ba-4e47-85ae-282184193b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of Dorothy and the Wizard in Oz\n",
      "    \n",
      "This ebook is for the use of anyon\n"
     ]
    }
   ],
   "source": [
    "print(decode([91, 48, 65, 62, 1, 44, 75, 72, 67, 62, 60, 77, 1, 35, 78, 77, 62, 71, 59, 62, 75, 64, 1, 62, 30, 72, 72, 68, 1, 72, 63, 1, 32, 72, 75, 72, 77, 65, 82, 1, 58, 71, 61, 1, 77, 65, 62, 1, 51, 66, 83, 58, 75, 61, 1, 66, 71, 1, 43, 83, 0, 1, 1, 1, 1, 0, 48, 65, 66, 76, 1, 62, 59, 72, 72, 68, 1, 66, 76, 1, 63, 72, 75, 1, 77, 65, 62, 1, 78, 76, 62, 1, 72, 63, 1, 58, 71, 82, 72, 71]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c28f70e-8edc-414b-bb10-d86ebbad0ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([91, 48, 65, 62,  1, 44, 75, 72, 67, 62, 60, 77,  1, 35, 78, 77, 62, 71,\n",
      "        59, 62, 75, 64,  1, 62, 30, 72, 72, 68,  1, 72, 63,  1, 32, 72, 75, 72,\n",
      "        77, 65, 82,  1, 58, 71, 61,  1, 77, 65, 62,  1, 51, 66, 83, 58, 75, 61,\n",
      "         1, 66, 71,  1, 43, 83,  0,  1,  1,  1,  1,  0, 48, 65, 66, 76,  1, 62,\n",
      "        59, 72, 72, 68,  1, 66, 76,  1, 63, 72, 75,  1, 77, 65, 62,  1, 78, 76,\n",
      "        62,  1, 72, 63,  1, 58, 71, 82, 72, 71])\n"
     ]
    }
   ],
   "source": [
    "data=torch.tensor(encode(text),dtype=torch.long) # jo mapped charcters hai unko hamne ek tensor(kinda matrix) pe store kiya as of now\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320a8877-a8a0-4633-bbd1-9870943feca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "tensor([[66, 71, 64,  1, 58, 71, 61,  0],\n",
      "        [69, 66, 68, 62, 69, 82,  1, 77],\n",
      "        [78, 76, 69, 82,  1, 58, 69, 72],\n",
      "        [70, 66, 71, 78, 77, 62, 27,  1],\n",
      "        [ 1, 72, 75,  1, 69, 66, 77, 77],\n",
      "        [ 1, 79, 62, 75, 82,  1, 72, 69],\n",
      "        [12,  1, 76, 72,  1, 77, 65, 58],\n",
      "        [77, 76,  1, 80, 62, 75, 62,  1]], device='cuda:0')\n",
      "targets\n",
      "tensor([[71, 64,  1, 58, 71, 61,  0, 75],\n",
      "        [66, 68, 62, 69, 82,  1, 77, 72],\n",
      "        [76, 69, 82,  1, 58, 69, 72, 71],\n",
      "        [66, 71, 78, 77, 62, 27,  1, 59],\n",
      "        [72, 75,  1, 69, 66, 77, 77, 69],\n",
      "        [79, 62, 75, 82,  1, 72, 69, 61],\n",
      "        [ 1, 76, 72,  1, 77, 65, 58, 77],\n",
      "        [76,  1, 80, 62, 75, 62,  1, 58]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n=int(0.8*len(data))\n",
    "train_data=data[:n]\n",
    "val_data=data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # print(ix)\n",
    "    x = torch.stack([data[i:i+block_size]for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size]for i in ix])\n",
    "    x,y = x.to(device) , y.to(device)\n",
    "    return x,y\n",
    "x,y = get_batch('train')\n",
    "\n",
    "print('inputs')\n",
    "print(x)\n",
    "print('targets')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705e63be-34da-42ba-8a14-7b4d0eb2c7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is tensor tensor([91]) target is  tensor(48)\n",
      "input is tensor tensor([91, 48]) target is  tensor(65)\n",
      "input is tensor tensor([91, 48, 65]) target is  tensor(62)\n",
      "input is tensor tensor([91, 48, 65, 62]) target is  tensor(1)\n",
      "input is tensor tensor([91, 48, 65, 62,  1]) target is  tensor(44)\n",
      "input is tensor tensor([91, 48, 65, 62,  1, 44]) target is  tensor(75)\n",
      "input is tensor tensor([91, 48, 65, 62,  1, 44, 75]) target is  tensor(72)\n",
      "input is tensor tensor([91, 48, 65, 62,  1, 44, 75, 72]) target is  tensor(67)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x=train_data[:block_size] # 0 to 7\n",
    "y=train_data[1:block_size+1] # 1 to 8\n",
    "for t in range (block_size): # 0 to 7\n",
    "    context=x[:t+1] # 0 to t\n",
    "    target=y[t] # tth \n",
    "    print('input is tensor',context,'target is ',target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ada4482-66d7-4f2c-857a-6ef38a03ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is checking how well the model is doing (the loss) on both the training and validation data — but without changing the model. that is without optimizing\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x,y= get_batch(split)\n",
    "            logits,loss = model.forward(x,y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c95090-5a62-46eb-9979-05d4ad76b4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rL*Yq36_/R(﻿‘qyGDN1(mi%1l$RemAIZWmuDM“_wNk2F’W$•tH™I—D'WWUsAPFW•“#8—nK9mqx﻿1,nu9*o5t-Ajg,\"#f”﻿ •r\"vOE—,A)E-_AC4$CQ3.-2‘L3CEf W$CK•_.u'—“]S0lr-__BmDpn(]aMM[#s”3rLE0]c(H™qdDyw$u%F2—]gos,oWE;6\n",
      "gad2JoXBBlLFRw-8”bWeJkpJ5)aZZ_OFW$!?“exp&]GkH™Ma‘p(l”,‘•t]4Ih*6(Z%k [p6]Y0L'WU”IR&Y—b9s)cVd44!‘H9asd0rNhY[gd9n'Ny(T3 !?PM\n",
      "[vCQDfIj:”\n",
      "TtVMa#;H*x78H.]JkpcDVDd.]2a\n",
      "(Q49.T9.d—IRVLj#3T—]bPTsYFvjy&wt-UF—YEFH“IRg—Ax6”\n",
      ")Yh;5c98N;a7;Qgq3ytd“\n",
      "0r[ SEZqkc.”‘U*gdh(’Lii-“L\n",
      "8Pf!VF,IRSAAb5xN6mY2i$JG,DI'M;rNxyTka_CE‘y—3rRzcTg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "                                                                                                                                                                                          \n",
    "class BigramLanguageModel(nn.Module): # nn.module se inherit kar rahe hai chize\n",
    "    def __init__(self, vocab_size): # init predefined chiz hai to initialize the BLM\n",
    "        super().__init__() # to initialize nn.module ye basically nn.module (parent class) ke constructor ko call karta hai\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # self.jaruri hai ye ek predefined chiz hai jo ek class ki khud ki chize store karwane and infact access karwane me help karta hai\n",
    "    \n",
    "    def forward(self, idx, targets=None): # ab ham learnable parameters ko train karenge ....\n",
    "        logits = self.token_embedding_table(idx) # (basically here we played smart and did vector embeddings = logits)(tabhi embedding table vocab*vocab ka bana hai)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # common parameters in ML b = batch size, T = time/sequence length, C = vocab size\n",
    "            logits = logits.view(B*T, C) # .view(), it's used to reshape a tensor without copying data.\n",
    "            targets = targets.view(B*T) # total number of tokens = B*T\n",
    "            loss = F.cross_entropy(logits, targets) \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens): # Use underscore for unused variable\n",
    "            logits, loss = self.forward(idx)\n",
    "            logits = logits[:, -1, :] # negative indexing on block size (T) - get last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc250de4-e191-40db-920b-2b96a68a93dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = : 0 ,loss {'train': tensor(4.9889), 'val': tensor(4.9879)}\n",
      "step = : 250 ,loss {'train': tensor(4.9063), 'val': tensor(4.9164)}\n",
      "step = : 500 ,loss {'train': tensor(4.8495), 'val': tensor(4.8260)}\n",
      "step = : 750 ,loss {'train': tensor(4.7670), 'val': tensor(4.7680)}\n",
      "step = : 1000 ,loss {'train': tensor(4.7027), 'val': tensor(4.6935)}\n",
      "step = : 1250 ,loss {'train': tensor(4.6087), 'val': tensor(4.6330)}\n",
      "step = : 1500 ,loss {'train': tensor(4.5606), 'val': tensor(4.5696)}\n",
      "step = : 1750 ,loss {'train': tensor(4.4875), 'val': tensor(4.4995)}\n",
      "step = : 2000 ,loss {'train': tensor(4.4390), 'val': tensor(4.4284)}\n",
      "step = : 2250 ,loss {'train': tensor(4.3625), 'val': tensor(4.3837)}\n",
      "step = : 2500 ,loss {'train': tensor(4.3086), 'val': tensor(4.3145)}\n",
      "step = : 2750 ,loss {'train': tensor(4.2418), 'val': tensor(4.2409)}\n",
      "step = : 3000 ,loss {'train': tensor(4.1703), 'val': tensor(4.2014)}\n",
      "step = : 3250 ,loss {'train': tensor(4.1355), 'val': tensor(4.1457)}\n",
      "step = : 3500 ,loss {'train': tensor(4.0567), 'val': tensor(4.0840)}\n",
      "step = : 3750 ,loss {'train': tensor(4.0123), 'val': tensor(4.0489)}\n",
      "step = : 4000 ,loss {'train': tensor(3.9433), 'val': tensor(3.9747)}\n",
      "step = : 4250 ,loss {'train': tensor(3.8943), 'val': tensor(3.9500)}\n",
      "step = : 4500 ,loss {'train': tensor(3.8502), 'val': tensor(3.8903)}\n",
      "step = : 4750 ,loss {'train': tensor(3.7978), 'val': tensor(3.8361)}\n",
      "step = : 5000 ,loss {'train': tensor(3.7422), 'val': tensor(3.7897)}\n",
      "step = : 5250 ,loss {'train': tensor(3.7114), 'val': tensor(3.7397)}\n",
      "step = : 5500 ,loss {'train': tensor(3.6433), 'val': tensor(3.7100)}\n",
      "step = : 5750 ,loss {'train': tensor(3.5966), 'val': tensor(3.6795)}\n",
      "step = : 6000 ,loss {'train': tensor(3.5650), 'val': tensor(3.6184)}\n",
      "step = : 6250 ,loss {'train': tensor(3.5241), 'val': tensor(3.5781)}\n",
      "step = : 6500 ,loss {'train': tensor(3.4887), 'val': tensor(3.5556)}\n",
      "step = : 6750 ,loss {'train': tensor(3.4476), 'val': tensor(3.5142)}\n",
      "step = : 7000 ,loss {'train': tensor(3.3953), 'val': tensor(3.4763)}\n",
      "step = : 7250 ,loss {'train': tensor(3.3884), 'val': tensor(3.4474)}\n",
      "step = : 7500 ,loss {'train': tensor(3.3341), 'val': tensor(3.4249)}\n",
      "step = : 7750 ,loss {'train': tensor(3.3025), 'val': tensor(3.3764)}\n",
      "step = : 8000 ,loss {'train': tensor(3.2680), 'val': tensor(3.3659)}\n",
      "step = : 8250 ,loss {'train': tensor(3.2378), 'val': tensor(3.3129)}\n",
      "step = : 8500 ,loss {'train': tensor(3.2049), 'val': tensor(3.2902)}\n",
      "step = : 8750 ,loss {'train': tensor(3.1767), 'val': tensor(3.2673)}\n",
      "step = : 9000 ,loss {'train': tensor(3.1458), 'val': tensor(3.2441)}\n",
      "step = : 9250 ,loss {'train': tensor(3.1156), 'val': tensor(3.2153)}\n",
      "step = : 9500 ,loss {'train': tensor(3.0928), 'val': tensor(3.2087)}\n",
      "step = : 9750 ,loss {'train': tensor(3.0592), 'val': tensor(3.1739)}\n",
      "step = : 10000 ,loss {'train': tensor(3.0476), 'val': tensor(3.1446)}\n",
      "step = : 10250 ,loss {'train': tensor(3.0250), 'val': tensor(3.1166)}\n",
      "step = : 10500 ,loss {'train': tensor(2.9940), 'val': tensor(3.0971)}\n",
      "step = : 10750 ,loss {'train': tensor(2.9766), 'val': tensor(3.0729)}\n",
      "step = : 11000 ,loss {'train': tensor(2.9586), 'val': tensor(3.0607)}\n",
      "step = : 11250 ,loss {'train': tensor(2.9303), 'val': tensor(3.0459)}\n",
      "step = : 11500 ,loss {'train': tensor(2.9172), 'val': tensor(3.0237)}\n",
      "step = : 11750 ,loss {'train': tensor(2.8975), 'val': tensor(3.0045)}\n",
      "step = : 12000 ,loss {'train': tensor(2.8855), 'val': tensor(3.0096)}\n",
      "step = : 12250 ,loss {'train': tensor(2.8514), 'val': tensor(2.9758)}\n",
      "step = : 12500 ,loss {'train': tensor(2.8414), 'val': tensor(2.9708)}\n",
      "step = : 12750 ,loss {'train': tensor(2.8413), 'val': tensor(2.9853)}\n",
      "step = : 13000 ,loss {'train': tensor(2.8227), 'val': tensor(2.9331)}\n",
      "step = : 13250 ,loss {'train': tensor(2.8220), 'val': tensor(2.9335)}\n",
      "step = : 13500 ,loss {'train': tensor(2.7933), 'val': tensor(2.9151)}\n",
      "step = : 13750 ,loss {'train': tensor(2.7811), 'val': tensor(2.9061)}\n",
      "step = : 14000 ,loss {'train': tensor(2.7729), 'val': tensor(2.9096)}\n",
      "step = : 14250 ,loss {'train': tensor(2.7554), 'val': tensor(2.8811)}\n",
      "step = : 14500 ,loss {'train': tensor(2.7358), 'val': tensor(2.8822)}\n",
      "step = : 14750 ,loss {'train': tensor(2.7187), 'val': tensor(2.8667)}\n",
      "step = : 15000 ,loss {'train': tensor(2.7037), 'val': tensor(2.8514)}\n",
      "step = : 15250 ,loss {'train': tensor(2.7137), 'val': tensor(2.8182)}\n",
      "step = : 15500 ,loss {'train': tensor(2.6972), 'val': tensor(2.8231)}\n",
      "step = : 15750 ,loss {'train': tensor(2.6835), 'val': tensor(2.8246)}\n",
      "step = : 16000 ,loss {'train': tensor(2.6886), 'val': tensor(2.8038)}\n",
      "step = : 16250 ,loss {'train': tensor(2.6626), 'val': tensor(2.7944)}\n",
      "step = : 16500 ,loss {'train': tensor(2.6701), 'val': tensor(2.8360)}\n",
      "step = : 16750 ,loss {'train': tensor(2.6657), 'val': tensor(2.7939)}\n",
      "step = : 17000 ,loss {'train': tensor(2.6587), 'val': tensor(2.7865)}\n",
      "step = : 17250 ,loss {'train': tensor(2.6139), 'val': tensor(2.7696)}\n",
      "step = : 17500 ,loss {'train': tensor(2.6545), 'val': tensor(2.7664)}\n",
      "step = : 17750 ,loss {'train': tensor(2.6380), 'val': tensor(2.7699)}\n",
      "step = : 18000 ,loss {'train': tensor(2.6486), 'val': tensor(2.7499)}\n",
      "step = : 18250 ,loss {'train': tensor(2.6257), 'val': tensor(2.7584)}\n",
      "step = : 18500 ,loss {'train': tensor(2.6147), 'val': tensor(2.7537)}\n",
      "step = : 18750 ,loss {'train': tensor(2.5897), 'val': tensor(2.7503)}\n",
      "step = : 19000 ,loss {'train': tensor(2.5965), 'val': tensor(2.7304)}\n",
      "step = : 19250 ,loss {'train': tensor(2.5947), 'val': tensor(2.7521)}\n",
      "step = : 19500 ,loss {'train': tensor(2.5920), 'val': tensor(2.7302)}\n",
      "step = : 19750 ,loss {'train': tensor(2.5791), 'val': tensor(2.7483)}\n",
      "step = : 20000 ,loss {'train': tensor(2.5940), 'val': tensor(2.7122)}\n",
      "step = : 20250 ,loss {'train': tensor(2.6080), 'val': tensor(2.7319)}\n",
      "step = : 20500 ,loss {'train': tensor(2.5734), 'val': tensor(2.7359)}\n",
      "step = : 20750 ,loss {'train': tensor(2.5722), 'val': tensor(2.7178)}\n",
      "step = : 21000 ,loss {'train': tensor(2.5592), 'val': tensor(2.7240)}\n",
      "step = : 21250 ,loss {'train': tensor(2.5720), 'val': tensor(2.7019)}\n",
      "step = : 21500 ,loss {'train': tensor(2.5646), 'val': tensor(2.7140)}\n",
      "step = : 21750 ,loss {'train': tensor(2.5590), 'val': tensor(2.7046)}\n",
      "step = : 22000 ,loss {'train': tensor(2.5569), 'val': tensor(2.7102)}\n",
      "step = : 22250 ,loss {'train': tensor(2.5264), 'val': tensor(2.7051)}\n",
      "step = : 22500 ,loss {'train': tensor(2.5477), 'val': tensor(2.6667)}\n",
      "step = : 22750 ,loss {'train': tensor(2.5290), 'val': tensor(2.6979)}\n",
      "step = : 23000 ,loss {'train': tensor(2.5426), 'val': tensor(2.6954)}\n",
      "step = : 23250 ,loss {'train': tensor(2.5393), 'val': tensor(2.7043)}\n",
      "step = : 23500 ,loss {'train': tensor(2.5375), 'val': tensor(2.6868)}\n",
      "step = : 23750 ,loss {'train': tensor(2.5162), 'val': tensor(2.6883)}\n",
      "step = : 24000 ,loss {'train': tensor(2.5147), 'val': tensor(2.6901)}\n",
      "step = : 24250 ,loss {'train': tensor(2.5201), 'val': tensor(2.7006)}\n",
      "step = : 24500 ,loss {'train': tensor(2.5220), 'val': tensor(2.7030)}\n",
      "step = : 24750 ,loss {'train': tensor(2.5109), 'val': tensor(2.6715)}\n",
      "step = : 25000 ,loss {'train': tensor(2.5191), 'val': tensor(2.6806)}\n",
      "step = : 25250 ,loss {'train': tensor(2.5333), 'val': tensor(2.6658)}\n",
      "step = : 25500 ,loss {'train': tensor(2.5225), 'val': tensor(2.6809)}\n",
      "step = : 25750 ,loss {'train': tensor(2.5352), 'val': tensor(2.6910)}\n",
      "step = : 26000 ,loss {'train': tensor(2.5142), 'val': tensor(2.6726)}\n",
      "step = : 26250 ,loss {'train': tensor(2.4962), 'val': tensor(2.6748)}\n",
      "step = : 26500 ,loss {'train': tensor(2.5197), 'val': tensor(2.6603)}\n",
      "step = : 26750 ,loss {'train': tensor(2.5003), 'val': tensor(2.6533)}\n",
      "step = : 27000 ,loss {'train': tensor(2.5053), 'val': tensor(2.6570)}\n",
      "step = : 27250 ,loss {'train': tensor(2.4888), 'val': tensor(2.6882)}\n",
      "step = : 27500 ,loss {'train': tensor(2.5206), 'val': tensor(2.6506)}\n",
      "step = : 27750 ,loss {'train': tensor(2.4958), 'val': tensor(2.6592)}\n",
      "step = : 28000 ,loss {'train': tensor(2.4934), 'val': tensor(2.6423)}\n",
      "step = : 28250 ,loss {'train': tensor(2.5131), 'val': tensor(2.6618)}\n",
      "step = : 28500 ,loss {'train': tensor(2.4890), 'val': tensor(2.6630)}\n",
      "step = : 28750 ,loss {'train': tensor(2.5047), 'val': tensor(2.6552)}\n",
      "step = : 29000 ,loss {'train': tensor(2.4926), 'val': tensor(2.6453)}\n",
      "step = : 29250 ,loss {'train': tensor(2.4877), 'val': tensor(2.6512)}\n",
      "step = : 29500 ,loss {'train': tensor(2.4946), 'val': tensor(2.6779)}\n",
      "step = : 29750 ,loss {'train': tensor(2.4828), 'val': tensor(2.6397)}\n",
      "step = : 30000 ,loss {'train': tensor(2.4648), 'val': tensor(2.6521)}\n",
      "step = : 30250 ,loss {'train': tensor(2.5090), 'val': tensor(2.6464)}\n",
      "step = : 30500 ,loss {'train': tensor(2.4831), 'val': tensor(2.6581)}\n",
      "step = : 30750 ,loss {'train': tensor(2.5098), 'val': tensor(2.6741)}\n",
      "step = : 31000 ,loss {'train': tensor(2.4815), 'val': tensor(2.6483)}\n",
      "step = : 31250 ,loss {'train': tensor(2.4972), 'val': tensor(2.6396)}\n",
      "step = : 31500 ,loss {'train': tensor(2.4821), 'val': tensor(2.6512)}\n",
      "step = : 31750 ,loss {'train': tensor(2.4875), 'val': tensor(2.6616)}\n",
      "step = : 32000 ,loss {'train': tensor(2.4924), 'val': tensor(2.6658)}\n",
      "step = : 32250 ,loss {'train': tensor(2.4933), 'val': tensor(2.6660)}\n",
      "step = : 32500 ,loss {'train': tensor(2.4688), 'val': tensor(2.6490)}\n",
      "step = : 32750 ,loss {'train': tensor(2.4850), 'val': tensor(2.6445)}\n",
      "step = : 33000 ,loss {'train': tensor(2.4663), 'val': tensor(2.6646)}\n",
      "step = : 33250 ,loss {'train': tensor(2.4661), 'val': tensor(2.6547)}\n",
      "step = : 33500 ,loss {'train': tensor(2.4821), 'val': tensor(2.6438)}\n",
      "step = : 33750 ,loss {'train': tensor(2.4545), 'val': tensor(2.6482)}\n",
      "step = : 34000 ,loss {'train': tensor(2.4637), 'val': tensor(2.6337)}\n",
      "step = : 34250 ,loss {'train': tensor(2.4699), 'val': tensor(2.6498)}\n",
      "step = : 34500 ,loss {'train': tensor(2.4821), 'val': tensor(2.6432)}\n",
      "step = : 34750 ,loss {'train': tensor(2.4776), 'val': tensor(2.6314)}\n",
      "step = : 35000 ,loss {'train': tensor(2.4602), 'val': tensor(2.6430)}\n",
      "step = : 35250 ,loss {'train': tensor(2.4662), 'val': tensor(2.6684)}\n",
      "step = : 35500 ,loss {'train': tensor(2.4637), 'val': tensor(2.6365)}\n",
      "step = : 35750 ,loss {'train': tensor(2.4756), 'val': tensor(2.6536)}\n",
      "step = : 36000 ,loss {'train': tensor(2.4752), 'val': tensor(2.6392)}\n",
      "step = : 36250 ,loss {'train': tensor(2.4723), 'val': tensor(2.6521)}\n",
      "step = : 36500 ,loss {'train': tensor(2.4701), 'val': tensor(2.6358)}\n",
      "step = : 36750 ,loss {'train': tensor(2.4549), 'val': tensor(2.6372)}\n",
      "step = : 37000 ,loss {'train': tensor(2.4521), 'val': tensor(2.6628)}\n",
      "step = : 37250 ,loss {'train': tensor(2.4593), 'val': tensor(2.6445)}\n",
      "step = : 37500 ,loss {'train': tensor(2.4544), 'val': tensor(2.6353)}\n",
      "step = : 37750 ,loss {'train': tensor(2.4658), 'val': tensor(2.6251)}\n",
      "step = : 38000 ,loss {'train': tensor(2.4602), 'val': tensor(2.6477)}\n",
      "step = : 38250 ,loss {'train': tensor(2.4726), 'val': tensor(2.6411)}\n",
      "step = : 38500 ,loss {'train': tensor(2.4505), 'val': tensor(2.6485)}\n",
      "step = : 38750 ,loss {'train': tensor(2.4505), 'val': tensor(2.6650)}\n",
      "step = : 39000 ,loss {'train': tensor(2.4732), 'val': tensor(2.6323)}\n",
      "step = : 39250 ,loss {'train': tensor(2.4670), 'val': tensor(2.6332)}\n",
      "step = : 39500 ,loss {'train': tensor(2.4645), 'val': tensor(2.6322)}\n",
      "step = : 39750 ,loss {'train': tensor(2.4560), 'val': tensor(2.6383)}\n",
      "step = : 40000 ,loss {'train': tensor(2.4663), 'val': tensor(2.6290)}\n",
      "step = : 40250 ,loss {'train': tensor(2.4680), 'val': tensor(2.6455)}\n",
      "step = : 40500 ,loss {'train': tensor(2.4634), 'val': tensor(2.6426)}\n",
      "step = : 40750 ,loss {'train': tensor(2.4590), 'val': tensor(2.6558)}\n",
      "step = : 41000 ,loss {'train': tensor(2.4554), 'val': tensor(2.6476)}\n",
      "step = : 41250 ,loss {'train': tensor(2.4556), 'val': tensor(2.5913)}\n",
      "step = : 41500 ,loss {'train': tensor(2.4663), 'val': tensor(2.6389)}\n",
      "step = : 41750 ,loss {'train': tensor(2.4410), 'val': tensor(2.6751)}\n",
      "step = : 42000 ,loss {'train': tensor(2.4515), 'val': tensor(2.6354)}\n",
      "step = : 42250 ,loss {'train': tensor(2.4580), 'val': tensor(2.6132)}\n",
      "step = : 42500 ,loss {'train': tensor(2.4442), 'val': tensor(2.6219)}\n",
      "step = : 42750 ,loss {'train': tensor(2.4464), 'val': tensor(2.6612)}\n",
      "step = : 43000 ,loss {'train': tensor(2.4553), 'val': tensor(2.6414)}\n",
      "step = : 43250 ,loss {'train': tensor(2.4428), 'val': tensor(2.6272)}\n",
      "step = : 43500 ,loss {'train': tensor(2.4601), 'val': tensor(2.6349)}\n",
      "step = : 43750 ,loss {'train': tensor(2.4576), 'val': tensor(2.6207)}\n",
      "step = : 44000 ,loss {'train': tensor(2.4393), 'val': tensor(2.6336)}\n",
      "step = : 44250 ,loss {'train': tensor(2.4353), 'val': tensor(2.6276)}\n",
      "step = : 44500 ,loss {'train': tensor(2.4436), 'val': tensor(2.6338)}\n",
      "step = : 44750 ,loss {'train': tensor(2.4487), 'val': tensor(2.6293)}\n",
      "step = : 45000 ,loss {'train': tensor(2.4515), 'val': tensor(2.6338)}\n",
      "step = : 45250 ,loss {'train': tensor(2.4427), 'val': tensor(2.6451)}\n",
      "step = : 45500 ,loss {'train': tensor(2.4544), 'val': tensor(2.6321)}\n",
      "step = : 45750 ,loss {'train': tensor(2.4578), 'val': tensor(2.6371)}\n",
      "step = : 46000 ,loss {'train': tensor(2.4498), 'val': tensor(2.6352)}\n",
      "step = : 46250 ,loss {'train': tensor(2.4461), 'val': tensor(2.6344)}\n",
      "step = : 46500 ,loss {'train': tensor(2.4445), 'val': tensor(2.6347)}\n",
      "step = : 46750 ,loss {'train': tensor(2.4403), 'val': tensor(2.6479)}\n",
      "step = : 47000 ,loss {'train': tensor(2.4656), 'val': tensor(2.6536)}\n",
      "step = : 47250 ,loss {'train': tensor(2.4403), 'val': tensor(2.6346)}\n",
      "step = : 47500 ,loss {'train': tensor(2.4530), 'val': tensor(2.6536)}\n",
      "step = : 47750 ,loss {'train': tensor(2.4499), 'val': tensor(2.6269)}\n",
      "step = : 48000 ,loss {'train': tensor(2.4536), 'val': tensor(2.6477)}\n",
      "step = : 48250 ,loss {'train': tensor(2.4327), 'val': tensor(2.6554)}\n",
      "step = : 48500 ,loss {'train': tensor(2.4376), 'val': tensor(2.6484)}\n",
      "step = : 48750 ,loss {'train': tensor(2.4359), 'val': tensor(2.6215)}\n",
      "step = : 49000 ,loss {'train': tensor(2.4506), 'val': tensor(2.6355)}\n",
      "step = : 49250 ,loss {'train': tensor(2.4362), 'val': tensor(2.6189)}\n",
      "step = : 49500 ,loss {'train': tensor(2.4416), 'val': tensor(2.6262)}\n",
      "step = : 49750 ,loss {'train': tensor(2.4349), 'val': tensor(2.6691)}\n",
      "step = : 50000 ,loss {'train': tensor(2.4421), 'val': tensor(2.6499)}\n",
      "step = : 50250 ,loss {'train': tensor(2.4532), 'val': tensor(2.6345)}\n",
      "step = : 50500 ,loss {'train': tensor(2.4317), 'val': tensor(2.6172)}\n",
      "step = : 50750 ,loss {'train': tensor(2.4380), 'val': tensor(2.6576)}\n",
      "step = : 51000 ,loss {'train': tensor(2.4577), 'val': tensor(2.6457)}\n",
      "step = : 51250 ,loss {'train': tensor(2.4292), 'val': tensor(2.6512)}\n",
      "step = : 51500 ,loss {'train': tensor(2.4471), 'val': tensor(2.6522)}\n",
      "step = : 51750 ,loss {'train': tensor(2.4451), 'val': tensor(2.6316)}\n",
      "step = : 52000 ,loss {'train': tensor(2.4388), 'val': tensor(2.6290)}\n",
      "step = : 52250 ,loss {'train': tensor(2.4385), 'val': tensor(2.6438)}\n",
      "step = : 52500 ,loss {'train': tensor(2.4343), 'val': tensor(2.6594)}\n",
      "step = : 52750 ,loss {'train': tensor(2.4311), 'val': tensor(2.6372)}\n",
      "step = : 53000 ,loss {'train': tensor(2.4389), 'val': tensor(2.6700)}\n",
      "step = : 53250 ,loss {'train': tensor(2.4486), 'val': tensor(2.6662)}\n",
      "step = : 53500 ,loss {'train': tensor(2.4479), 'val': tensor(2.6447)}\n",
      "step = : 53750 ,loss {'train': tensor(2.4274), 'val': tensor(2.6271)}\n",
      "step = : 54000 ,loss {'train': tensor(2.4375), 'val': tensor(2.6214)}\n",
      "step = : 54250 ,loss {'train': tensor(2.4381), 'val': tensor(2.6467)}\n",
      "step = : 54500 ,loss {'train': tensor(2.4437), 'val': tensor(2.6342)}\n",
      "step = : 54750 ,loss {'train': tensor(2.4500), 'val': tensor(2.6170)}\n",
      "step = : 55000 ,loss {'train': tensor(2.4475), 'val': tensor(2.6383)}\n",
      "step = : 55250 ,loss {'train': tensor(2.4374), 'val': tensor(2.6837)}\n",
      "step = : 55500 ,loss {'train': tensor(2.4301), 'val': tensor(2.6585)}\n",
      "step = : 55750 ,loss {'train': tensor(2.4363), 'val': tensor(2.6402)}\n",
      "step = : 56000 ,loss {'train': tensor(2.4490), 'val': tensor(2.6691)}\n",
      "step = : 56250 ,loss {'train': tensor(2.4399), 'val': tensor(2.6472)}\n",
      "step = : 56500 ,loss {'train': tensor(2.4304), 'val': tensor(2.6463)}\n",
      "step = : 56750 ,loss {'train': tensor(2.4606), 'val': tensor(2.6362)}\n",
      "step = : 57000 ,loss {'train': tensor(2.4323), 'val': tensor(2.6383)}\n",
      "step = : 57250 ,loss {'train': tensor(2.4401), 'val': tensor(2.6676)}\n",
      "step = : 57500 ,loss {'train': tensor(2.4444), 'val': tensor(2.6327)}\n",
      "step = : 57750 ,loss {'train': tensor(2.4350), 'val': tensor(2.6271)}\n",
      "step = : 58000 ,loss {'train': tensor(2.4300), 'val': tensor(2.6250)}\n",
      "step = : 58250 ,loss {'train': tensor(2.4476), 'val': tensor(2.6161)}\n",
      "step = : 58500 ,loss {'train': tensor(2.4481), 'val': tensor(2.6529)}\n",
      "step = : 58750 ,loss {'train': tensor(2.4421), 'val': tensor(2.6315)}\n",
      "step = : 59000 ,loss {'train': tensor(2.4549), 'val': tensor(2.6155)}\n",
      "step = : 59250 ,loss {'train': tensor(2.4404), 'val': tensor(2.6202)}\n",
      "step = : 59500 ,loss {'train': tensor(2.4288), 'val': tensor(2.6457)}\n",
      "step = : 59750 ,loss {'train': tensor(2.4236), 'val': tensor(2.6532)}\n",
      "step = : 60000 ,loss {'train': tensor(2.4270), 'val': tensor(2.6264)}\n",
      "step = : 60250 ,loss {'train': tensor(2.4410), 'val': tensor(2.6571)}\n",
      "step = : 60500 ,loss {'train': tensor(2.4258), 'val': tensor(2.6264)}\n",
      "step = : 60750 ,loss {'train': tensor(2.4449), 'val': tensor(2.6681)}\n",
      "step = : 61000 ,loss {'train': tensor(2.4355), 'val': tensor(2.6131)}\n",
      "step = : 61250 ,loss {'train': tensor(2.4355), 'val': tensor(2.6426)}\n",
      "step = : 61500 ,loss {'train': tensor(2.4290), 'val': tensor(2.6253)}\n",
      "step = : 61750 ,loss {'train': tensor(2.4435), 'val': tensor(2.6401)}\n",
      "step = : 62000 ,loss {'train': tensor(2.4365), 'val': tensor(2.6341)}\n",
      "step = : 62250 ,loss {'train': tensor(2.4306), 'val': tensor(2.6391)}\n",
      "step = : 62500 ,loss {'train': tensor(2.4388), 'val': tensor(2.6339)}\n",
      "step = : 62750 ,loss {'train': tensor(2.4583), 'val': tensor(2.6542)}\n",
      "step = : 63000 ,loss {'train': tensor(2.4272), 'val': tensor(2.6324)}\n",
      "step = : 63250 ,loss {'train': tensor(2.4251), 'val': tensor(2.6354)}\n",
      "step = : 63500 ,loss {'train': tensor(2.4436), 'val': tensor(2.6619)}\n",
      "step = : 63750 ,loss {'train': tensor(2.4270), 'val': tensor(2.6369)}\n",
      "step = : 64000 ,loss {'train': tensor(2.4425), 'val': tensor(2.6193)}\n",
      "step = : 64250 ,loss {'train': tensor(2.4513), 'val': tensor(2.6610)}\n",
      "step = : 64500 ,loss {'train': tensor(2.4344), 'val': tensor(2.6410)}\n",
      "step = : 64750 ,loss {'train': tensor(2.4338), 'val': tensor(2.6247)}\n",
      "step = : 65000 ,loss {'train': tensor(2.4309), 'val': tensor(2.6496)}\n",
      "step = : 65250 ,loss {'train': tensor(2.4538), 'val': tensor(2.6425)}\n",
      "step = : 65500 ,loss {'train': tensor(2.4299), 'val': tensor(2.6544)}\n",
      "step = : 65750 ,loss {'train': tensor(2.4419), 'val': tensor(2.5946)}\n",
      "step = : 66000 ,loss {'train': tensor(2.4467), 'val': tensor(2.6364)}\n",
      "step = : 66250 ,loss {'train': tensor(2.4423), 'val': tensor(2.6389)}\n",
      "step = : 66500 ,loss {'train': tensor(2.4360), 'val': tensor(2.6314)}\n",
      "step = : 66750 ,loss {'train': tensor(2.4355), 'val': tensor(2.6153)}\n",
      "step = : 67000 ,loss {'train': tensor(2.4405), 'val': tensor(2.6540)}\n",
      "step = : 67250 ,loss {'train': tensor(2.4420), 'val': tensor(2.6593)}\n",
      "step = : 67500 ,loss {'train': tensor(2.4477), 'val': tensor(2.6533)}\n",
      "step = : 67750 ,loss {'train': tensor(2.4580), 'val': tensor(2.6654)}\n",
      "step = : 68000 ,loss {'train': tensor(2.4532), 'val': tensor(2.6254)}\n",
      "step = : 68250 ,loss {'train': tensor(2.4292), 'val': tensor(2.6421)}\n",
      "step = : 68500 ,loss {'train': tensor(2.4245), 'val': tensor(2.6349)}\n",
      "step = : 68750 ,loss {'train': tensor(2.4448), 'val': tensor(2.6457)}\n",
      "step = : 69000 ,loss {'train': tensor(2.4404), 'val': tensor(2.6412)}\n",
      "step = : 69250 ,loss {'train': tensor(2.4454), 'val': tensor(2.6433)}\n",
      "step = : 69500 ,loss {'train': tensor(2.4348), 'val': tensor(2.6452)}\n",
      "step = : 69750 ,loss {'train': tensor(2.4297), 'val': tensor(2.6339)}\n",
      "step = : 70000 ,loss {'train': tensor(2.4374), 'val': tensor(2.6657)}\n",
      "step = : 70250 ,loss {'train': tensor(2.4401), 'val': tensor(2.6522)}\n",
      "step = : 70500 ,loss {'train': tensor(2.4386), 'val': tensor(2.6522)}\n",
      "step = : 70750 ,loss {'train': tensor(2.4427), 'val': tensor(2.6401)}\n",
      "step = : 71000 ,loss {'train': tensor(2.4475), 'val': tensor(2.6640)}\n",
      "step = : 71250 ,loss {'train': tensor(2.4100), 'val': tensor(2.6269)}\n",
      "step = : 71500 ,loss {'train': tensor(2.4116), 'val': tensor(2.6529)}\n",
      "step = : 71750 ,loss {'train': tensor(2.4329), 'val': tensor(2.6455)}\n",
      "step = : 72000 ,loss {'train': tensor(2.4465), 'val': tensor(2.6275)}\n",
      "step = : 72250 ,loss {'train': tensor(2.4403), 'val': tensor(2.6316)}\n",
      "step = : 72500 ,loss {'train': tensor(2.4612), 'val': tensor(2.6590)}\n",
      "step = : 72750 ,loss {'train': tensor(2.4299), 'val': tensor(2.6447)}\n",
      "step = : 73000 ,loss {'train': tensor(2.4349), 'val': tensor(2.6154)}\n",
      "step = : 73250 ,loss {'train': tensor(2.4429), 'val': tensor(2.6276)}\n",
      "step = : 73500 ,loss {'train': tensor(2.4366), 'val': tensor(2.6633)}\n",
      "step = : 73750 ,loss {'train': tensor(2.4304), 'val': tensor(2.6629)}\n",
      "step = : 74000 ,loss {'train': tensor(2.4415), 'val': tensor(2.6649)}\n",
      "step = : 74250 ,loss {'train': tensor(2.4309), 'val': tensor(2.6498)}\n",
      "step = : 74500 ,loss {'train': tensor(2.4239), 'val': tensor(2.6261)}\n",
      "step = : 74750 ,loss {'train': tensor(2.4374), 'val': tensor(2.6504)}\n",
      "step = : 75000 ,loss {'train': tensor(2.4395), 'val': tensor(2.6833)}\n",
      "step = : 75250 ,loss {'train': tensor(2.4228), 'val': tensor(2.6633)}\n",
      "step = : 75500 ,loss {'train': tensor(2.4234), 'val': tensor(2.6321)}\n",
      "step = : 75750 ,loss {'train': tensor(2.4388), 'val': tensor(2.6412)}\n",
      "step = : 76000 ,loss {'train': tensor(2.4352), 'val': tensor(2.6543)}\n",
      "step = : 76250 ,loss {'train': tensor(2.4368), 'val': tensor(2.6413)}\n",
      "step = : 76500 ,loss {'train': tensor(2.4453), 'val': tensor(2.6660)}\n",
      "step = : 76750 ,loss {'train': tensor(2.4263), 'val': tensor(2.6324)}\n",
      "step = : 77000 ,loss {'train': tensor(2.4491), 'val': tensor(2.6503)}\n",
      "step = : 77250 ,loss {'train': tensor(2.4462), 'val': tensor(2.6740)}\n",
      "step = : 77500 ,loss {'train': tensor(2.4422), 'val': tensor(2.6727)}\n",
      "step = : 77750 ,loss {'train': tensor(2.4360), 'val': tensor(2.6375)}\n",
      "step = : 78000 ,loss {'train': tensor(2.4267), 'val': tensor(2.6506)}\n",
      "step = : 78250 ,loss {'train': tensor(2.4595), 'val': tensor(2.6433)}\n",
      "step = : 78500 ,loss {'train': tensor(2.4496), 'val': tensor(2.6576)}\n",
      "step = : 78750 ,loss {'train': tensor(2.4301), 'val': tensor(2.6386)}\n",
      "step = : 79000 ,loss {'train': tensor(2.4302), 'val': tensor(2.6402)}\n",
      "step = : 79250 ,loss {'train': tensor(2.4253), 'val': tensor(2.6534)}\n",
      "step = : 79500 ,loss {'train': tensor(2.4208), 'val': tensor(2.6506)}\n",
      "step = : 79750 ,loss {'train': tensor(2.4302), 'val': tensor(2.6443)}\n",
      "step = : 80000 ,loss {'train': tensor(2.4244), 'val': tensor(2.6571)}\n",
      "step = : 80250 ,loss {'train': tensor(2.4503), 'val': tensor(2.6689)}\n",
      "step = : 80500 ,loss {'train': tensor(2.4467), 'val': tensor(2.6786)}\n",
      "step = : 80750 ,loss {'train': tensor(2.4345), 'val': tensor(2.6444)}\n",
      "step = : 81000 ,loss {'train': tensor(2.4371), 'val': tensor(2.6640)}\n",
      "step = : 81250 ,loss {'train': tensor(2.4438), 'val': tensor(2.6398)}\n",
      "step = : 81500 ,loss {'train': tensor(2.4261), 'val': tensor(2.6173)}\n",
      "step = : 81750 ,loss {'train': tensor(2.4260), 'val': tensor(2.6528)}\n",
      "step = : 82000 ,loss {'train': tensor(2.4376), 'val': tensor(2.6345)}\n",
      "step = : 82250 ,loss {'train': tensor(2.4365), 'val': tensor(2.6549)}\n",
      "step = : 82500 ,loss {'train': tensor(2.4245), 'val': tensor(2.6549)}\n",
      "step = : 82750 ,loss {'train': tensor(2.4209), 'val': tensor(2.6475)}\n",
      "step = : 83000 ,loss {'train': tensor(2.4325), 'val': tensor(2.6683)}\n",
      "step = : 83250 ,loss {'train': tensor(2.4366), 'val': tensor(2.6570)}\n",
      "step = : 83500 ,loss {'train': tensor(2.4326), 'val': tensor(2.6673)}\n",
      "step = : 83750 ,loss {'train': tensor(2.4427), 'val': tensor(2.6465)}\n",
      "step = : 84000 ,loss {'train': tensor(2.4388), 'val': tensor(2.6459)}\n",
      "step = : 84250 ,loss {'train': tensor(2.4200), 'val': tensor(2.6412)}\n",
      "step = : 84500 ,loss {'train': tensor(2.4326), 'val': tensor(2.6719)}\n",
      "step = : 84750 ,loss {'train': tensor(2.4209), 'val': tensor(2.6449)}\n",
      "step = : 85000 ,loss {'train': tensor(2.4403), 'val': tensor(2.6532)}\n",
      "step = : 85250 ,loss {'train': tensor(2.4481), 'val': tensor(2.6388)}\n",
      "step = : 85500 ,loss {'train': tensor(2.4411), 'val': tensor(2.6591)}\n",
      "step = : 85750 ,loss {'train': tensor(2.4319), 'val': tensor(2.6555)}\n",
      "step = : 86000 ,loss {'train': tensor(2.4447), 'val': tensor(2.6338)}\n",
      "step = : 86250 ,loss {'train': tensor(2.4145), 'val': tensor(2.6987)}\n",
      "step = : 86500 ,loss {'train': tensor(2.4380), 'val': tensor(2.6565)}\n",
      "step = : 86750 ,loss {'train': tensor(2.4495), 'val': tensor(2.6631)}\n",
      "step = : 87000 ,loss {'train': tensor(2.4381), 'val': tensor(2.6453)}\n",
      "step = : 87250 ,loss {'train': tensor(2.4076), 'val': tensor(2.6526)}\n",
      "step = : 87500 ,loss {'train': tensor(2.4248), 'val': tensor(2.6506)}\n",
      "step = : 87750 ,loss {'train': tensor(2.4528), 'val': tensor(2.6397)}\n",
      "step = : 88000 ,loss {'train': tensor(2.4621), 'val': tensor(2.6868)}\n",
      "step = : 88250 ,loss {'train': tensor(2.4396), 'val': tensor(2.6850)}\n",
      "step = : 88500 ,loss {'train': tensor(2.4192), 'val': tensor(2.6245)}\n",
      "step = : 88750 ,loss {'train': tensor(2.4364), 'val': tensor(2.6274)}\n",
      "step = : 89000 ,loss {'train': tensor(2.4198), 'val': tensor(2.6685)}\n",
      "step = : 89250 ,loss {'train': tensor(2.4374), 'val': tensor(2.6506)}\n",
      "step = : 89500 ,loss {'train': tensor(2.4289), 'val': tensor(2.6792)}\n",
      "step = : 89750 ,loss {'train': tensor(2.4125), 'val': tensor(2.6375)}\n",
      "step = : 90000 ,loss {'train': tensor(2.4157), 'val': tensor(2.6678)}\n",
      "step = : 90250 ,loss {'train': tensor(2.4285), 'val': tensor(2.6445)}\n",
      "step = : 90500 ,loss {'train': tensor(2.4344), 'val': tensor(2.6642)}\n",
      "step = : 90750 ,loss {'train': tensor(2.4380), 'val': tensor(2.6411)}\n",
      "step = : 91000 ,loss {'train': tensor(2.4386), 'val': tensor(2.6293)}\n",
      "step = : 91250 ,loss {'train': tensor(2.4375), 'val': tensor(2.6386)}\n",
      "step = : 91500 ,loss {'train': tensor(2.4442), 'val': tensor(2.6776)}\n",
      "step = : 91750 ,loss {'train': tensor(2.4492), 'val': tensor(2.6788)}\n",
      "step = : 92000 ,loss {'train': tensor(2.4360), 'val': tensor(2.6349)}\n",
      "step = : 92250 ,loss {'train': tensor(2.4085), 'val': tensor(2.6464)}\n",
      "step = : 92500 ,loss {'train': tensor(2.4262), 'val': tensor(2.6633)}\n",
      "step = : 92750 ,loss {'train': tensor(2.4419), 'val': tensor(2.6784)}\n",
      "step = : 93000 ,loss {'train': tensor(2.4189), 'val': tensor(2.6268)}\n",
      "step = : 93250 ,loss {'train': tensor(2.4372), 'val': tensor(2.6458)}\n",
      "step = : 93500 ,loss {'train': tensor(2.4218), 'val': tensor(2.6533)}\n",
      "step = : 93750 ,loss {'train': tensor(2.4265), 'val': tensor(2.6599)}\n",
      "step = : 94000 ,loss {'train': tensor(2.4375), 'val': tensor(2.6576)}\n",
      "step = : 94250 ,loss {'train': tensor(2.4286), 'val': tensor(2.6308)}\n",
      "step = : 94500 ,loss {'train': tensor(2.4263), 'val': tensor(2.6521)}\n",
      "step = : 94750 ,loss {'train': tensor(2.4314), 'val': tensor(2.6481)}\n",
      "step = : 95000 ,loss {'train': tensor(2.4568), 'val': tensor(2.6479)}\n",
      "step = : 95250 ,loss {'train': tensor(2.4312), 'val': tensor(2.6634)}\n",
      "step = : 95500 ,loss {'train': tensor(2.4262), 'val': tensor(2.6190)}\n",
      "step = : 95750 ,loss {'train': tensor(2.4126), 'val': tensor(2.6490)}\n",
      "step = : 96000 ,loss {'train': tensor(2.4430), 'val': tensor(2.6686)}\n",
      "step = : 96250 ,loss {'train': tensor(2.4244), 'val': tensor(2.6371)}\n",
      "step = : 96500 ,loss {'train': tensor(2.4402), 'val': tensor(2.6464)}\n",
      "step = : 96750 ,loss {'train': tensor(2.4284), 'val': tensor(2.6773)}\n",
      "step = : 97000 ,loss {'train': tensor(2.4321), 'val': tensor(2.6335)}\n",
      "step = : 97250 ,loss {'train': tensor(2.4210), 'val': tensor(2.6900)}\n",
      "step = : 97500 ,loss {'train': tensor(2.4030), 'val': tensor(2.6505)}\n",
      "step = : 97750 ,loss {'train': tensor(2.4475), 'val': tensor(2.6327)}\n",
      "step = : 98000 ,loss {'train': tensor(2.4317), 'val': tensor(2.6539)}\n",
      "step = : 98250 ,loss {'train': tensor(2.4231), 'val': tensor(2.6566)}\n",
      "step = : 98500 ,loss {'train': tensor(2.4186), 'val': tensor(2.6733)}\n",
      "step = : 98750 ,loss {'train': tensor(2.4203), 'val': tensor(2.6395)}\n",
      "step = : 99000 ,loss {'train': tensor(2.4296), 'val': tensor(2.6624)}\n",
      "step = : 99250 ,loss {'train': tensor(2.4285), 'val': tensor(2.6763)}\n",
      "step = : 99500 ,loss {'train': tensor(2.4455), 'val': tensor(2.6421)}\n",
      "step = : 99750 ,loss {'train': tensor(2.4362), 'val': tensor(2.6626)}\n",
      "2.6420912742614746\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "for iter in range (max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step = : {iter} ,loss {losses}')\n",
    "    xb , yb = get_batch('train')\n",
    "    logits,loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True) # hmm this step is just cleaning the mess for backpropagation\n",
    "    loss.backward() # back propagation \n",
    "    optimizer.step() # optimizing that is tuning in the opposite direction of gradient calculated by back prpagation\n",
    "print(loss.item()) # loss of the last iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f6a2c2-5f14-448f-949c-376d2193669b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Jid Wit, thefuswatotre r, icatwit.\n",
      "\n",
      "l, pp sthe e th\n",
      "cowe azat k lileshedol?\" be wony l ind d athe bubrorind ARoutens trcim as, weaps?\" ine opthtorous\n",
      "\n",
      "ws\n",
      "thet we ale wine t ar wlle brd.\"Alsm f aned weallory whenghed s, t e alo fr tts ZMaly l poraith asss n dove s Jon mair\n",
      "\n",
      "t alit aiganourooft wiowein ckin's meryome asif ouedagleer th Jiou Pe M y  herraind l imuthad ne the orod herrave tlene.\n",
      "\"HEI m, ins.\" I thur cy as,\" h wicans whather as we freser h; t the\n",
      "grutheedotheace t Dond e oche cavab.\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573bbcf6-a4a9-4956-835a-fa2e939d1322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-prompt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
